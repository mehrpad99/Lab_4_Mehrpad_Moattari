{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Coffee Lovers Unite (sort of)\"\n",
    "author: \"Mehrpad Moattari\"\n",
    "date: \"October 30, 2023\"\n",
    "font-family: \"serif\"\n",
    "fontcolor: \"steelblue\"\n",
    "backgroundcolor: \"whitesmoke\"\n",
    "self-contained: true\n",
    "format:\n",
    "  html:\n",
    "    theme: \"simplex\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Scrape data from menuism.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.menuism.com/restaurant-locations/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    chains = [\n",
    "        \"Starbucks\",\n",
    "        \"Dunkin Donuts\",\n",
    "        \"Peet's Coffee and Tea\",\n",
    "        \"Tim Horton's\",\n",
    "        \"Panera Bread\",\n",
    "        \"Caribou Coffee\",\n",
    "        \"Au Bon Pain\",\n",
    "        \"The Coffee Bean and Tea Leaf\",\n",
    "        \"McDonald's\",\n",
    "    ]\n",
    "\n",
    "   \n",
    "    data_dict = {\"State Name\": [], \"Number of Locations\": []}\n",
    "\n",
    "    for chain in chains:\n",
    "        elements = soup.find_all(\"span\", {\"class\": \"chainname\"}, text=chain)\n",
    "        total_locations = 0\n",
    "\n",
    "        for element in elements:\n",
    "            location_count = element.find_next(\"span\", {\"class\": \"num-locations\"})\n",
    "            if location_count:\n",
    "                total_locations += int(location_count.text.strip())\n",
    "                \n",
    "        data_dict[\"State Name\"].append(chain)\n",
    "        data_dict[\"Number of Locations\"].append(total_locations)\n",
    "\n",
    "    state_locations = pd.DataFrame(data_dict)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.stateabb() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateabb(state_name):\n",
    "    state_abbreviations=pd.read_csv()\n",
    "    abbreviation_series=state_abbreviations[state_abbreviations['State']==state_name]['State Abbreviation']\n",
    "\n",
    "    if not abbreviation_series.empty:\n",
    "        return abbreviation_series.values[0]\n",
    "    else:\n",
    "        return \"Not a State\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Parse, Merge, and Tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = {\n",
    "    'Starbucks': {'California': 500, 'Texas': 300, ...},\n",
    "    'Dunkin Donuts': {'California': 100, 'Texas': 200, ...},\n",
    "    ...\n",
    "}\n",
    "\n",
    "dfs = [pd.DataFrame.from_dict({chain: locations, 'State Name': locations.keys()}).set_index('State Name') for chain, locations in data_example.items()]\n",
    "\n",
    "df_merged = pd.concat(dfs, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.Scrape state names and populations from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://simple.wikipedia.org/wiki/List_of_U.S._states_by_population\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "    state_names = []\n",
    "    populations = []\n",
    "\n",
    "    for row in table.find_all(\"tr\")[1:]: \n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) >= 2:\n",
    "            state_name = columns[2].text.strip()\n",
    "            population = columns[3].text.strip()\n",
    "\n",
    "            state_names.append(state_name)\n",
    "            populations.append(population)\n",
    "\n",
    "    data = {\"State Name\": state_names, \"Population\": populations}\n",
    "    state_pop = pd.DataFrame(data)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage\")\n",
    "state_pop=state_pop.drop([56,57,58,59])\n",
    "state_pop['Population']=state_pop['Population'].str.replace(\",\",\"\",regex=True).astype(int)\n",
    "state_pop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The finacial data is made up \n",
    "financial_df = pd.DataFrame(financial_data)\n",
    "\n",
    "financial_df\n",
    "RESULT\n",
    "\n",
    "#                          Chain  Revenue ($M)\n",
    "#0                     Starbucks         26800\n",
    "#1                 Dunkin Donuts          1200\n",
    "#2         Peet's Coffee and Tea           700\n",
    "#3                  Tim Horton's          3000\n",
    "#4                  Panera Bread          2500\n",
    "#5                Caribou Coffee           350\n",
    "#6                   Au Bon Pain           400\n",
    "#7  The Coffee Bean and Tea Leaf           310\n",
    "# 8                    McDonald's         21000\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapping = {\n",
    "    \"Northeast\": [\"Connecticut\", \"Maine\", \"Massachusetts\", \"New Hampshire\", \"Rhode Island\", \n",
    "                  \"Vermont\", \"New Jersey\", \"New York\", \"Pennsylvania\"],\n",
    "    \"Midwest\": [\"Illinois\", \"Indiana\", \"Michigan\", \"Ohio\", \"Wisconsin\", \"Iowa\", \"Kansas\", \n",
    "                \"Minnesota\", \"Missouri\", \"Nebraska\", \"North Dakota\", \"South Dakota\"],\n",
    "    \"South\": [\"Delaware\", \"Florida\", \"Georgia\", \"Maryland\", \"North Carolina\", \"South Carolina\", \n",
    "              \"Virginia\", \"West Virginia\", \"Alabama\", \"Kentucky\", \"Mississippi\", \"Tennessee\", \n",
    "              \"Arkansas\", \"Louisiana\", \"Oklahoma\", \"Texas\"],\n",
    "    \"West\": [\"Arizona\", \"Colorado\", \"Idaho\", \"Montana\", \"Nevada\", \"New Mexico\", \"Utah\", \n",
    "             \"Wyoming\", \"Alaska\", \"California\", \"Hawaii\", \"Oregon\", \"Washington\"]\n",
    "}\n",
    "\n",
    "def get_region(state):\n",
    "    for region, states in region_mapping.items():\n",
    "        if state in states:\n",
    "            return region\n",
    "    return \"Unknown\"  \n",
    "\n",
    "# The region column\n",
    "states_data[\"Region\"] = states_data[\"State\"].apply(get_region)\n",
    "\n",
    "states_data[[\"State\", \"Region\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = states_data_cleaned.assign(key=1).merge(financial_df.assign(key=1), on='key').drop('key', axis=1)\n",
    "\n",
    "merged_df[\"Region\"] = merged_df[\"State Name\"].apply(get_region)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Are some of these chains more prevalent in certain states than others? Possibly despite having less stores overall? Same questions for regions instead of states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mehrp\\OneDrive\\Documents\\GitHub\\Lab_4_Mehrpad\\Lab_4_Mehrpad.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mehrp/OneDrive/Documents/GitHub/Lab_4_Mehrpad/Lab_4_Mehrpad.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m statewise_counts \u001b[39m=\u001b[39m merged_df\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mState Abbreviation\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mChain\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39msize()\u001b[39m.\u001b[39munstack()\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mehrp/OneDrive/Documents/GitHub/Lab_4_Mehrpad/Lab_4_Mehrpad.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m regionwise_counts \u001b[39m=\u001b[39m merged_df\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mRegion\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mChain\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39msize()\u001b[39m.\u001b[39munstack()\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mehrp/OneDrive/Documents/GitHub/Lab_4_Mehrpad/Lab_4_Mehrpad.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m overall_counts \u001b[39m=\u001b[39m merged_df[\u001b[39m'\u001b[39m\u001b[39mChain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# I am assuming this is how I would approach it: \n",
    "statewise_counts = merged_df.groupby(['State Abbreviation', 'Chain']).size().unstack().fillna(0)\n",
    "\n",
    "regionwise_counts = merged_df.groupby(['Region', 'Chain']).size().unstack().fillna(0)\n",
    "\n",
    "overall_counts = merged_df['Chain'].value_counts()\n",
    "\n",
    "statewise_counts, regionwise_counts, overall_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. How does your chosen financial metric change by state and region for each chain? For example, having 5 stores in California is very different from having 5 stores in Wyoming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am assuming this is how I would approach it: \n",
    "statewise_revenue = merged_df.groupby('State Abbreviation')['Revenue ($M)'].sum()\n",
    "\n",
    "regionwise_revenue = merged_df.groupby('Region')['Revenue ($M)'].sum()\n",
    "\n",
    "statewise_data = pd.merge(statewise_revenue, states_data_cleaned[['State Abbreviation', 'Population']], on=\"State Abbreviation\", how=\"left\")\n",
    "\n",
    "correlation = statewise_data['Population'].corr(statewise_data['Revenue ($M)'])\n",
    "\n",
    "statewise_revenue, regionwise_revenue, correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My best effort in creating this mega function: \n",
    "\n",
    "def scrape_menuism_data(url):\n",
    "    \"\"\"\n",
    "    Scrape the menuism webpage for state names and corresponding number of store locations.\n",
    "    \n",
    "    Parameters:\n",
    "    - url (str): The menuism URL for a specific coffee chain.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing state abbreviation, location count, and company name.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    data_dict = {\"State Abbreviation\": [], \"Location Count\": [], \"Company Name\": []}\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extracting company name from the URL\n",
    "        company_name = url.split(\"/\")[-2].replace(\"-\", \" \").title()\n",
    "        \n",
    "        elements = soup.find_all(\"span\", {\"class\": \"chainname\"}, text=company_name)\n",
    "\n",
    "        for element in elements:\n",
    "            location_count = element.find_next(\"span\", {\"class\": \"num-locations\"})\n",
    "            state_name = element.find_next(\"span\", {\"class\": \"statename\"}).text.strip()\n",
    "            \n",
    "            if location_count and state_name:\n",
    "                data_dict[\"State Abbreviation\"].append(stateabb(state_name))\n",
    "                data_dict[\"Location Count\"].append(int(location_count.text.strip()))\n",
    "                data_dict[\"Company Name\"].append(company_name)\n",
    "                \n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage\")\n",
    "\n",
    "    return pd.DataFrame(data_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
